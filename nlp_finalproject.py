# -*- coding: utf-8 -*-
"""nlp_finalproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kE1QUQNL78VL8NAatW0tEyYftYfk9bds

# Topic Extraction from Arabic Tweets - NLP project - Milestone

**Aya Joharji**

**Duaa Khorsheed**

**Howida Shikhoun**

**Methodology and Baseline:** TF-IDF(Term Frequency-Inverse Document Frequency)  and Word2vec combined  together with a linear classifie Logistic Regression in machine learning and LSTM in deep learning

**Implementation Platform:** Python with libraries NLTK (Natural Language Toolkit), Scikit-learn, and perhaps TensorFlow for methods(DL). Kaggle as a platform

## Collect and prepare data

### Data Cleaning:
"""

import pandas as pd
import numpy as np
import os
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import ISRIStemmer
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from tensorflow.keras.preprocessing.text import Tokenizer

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

#loading dataset
df= pd.read_csv('/kaggle/input/arabic-twitter-data-for-sentiment/all_data.csv',encoding = "utf-8-sig")
df

df.head(10)

df.info()

# Find missing data
missing_data = df.isnull().sum()

# Display missing data counts
print("Missing Data:")
print(missing_data)

# Replace 'sentiment' column values with 1 for 'pos' and 0 otherwise
df['sentiment'] = df['sentiment'].apply(lambda x: 1 if x == 'pos' else 0)

df

# Assuming 'df' is your DataFrame containing the dataset
sentiment_counts = df['sentiment'].value_counts()
print(sentiment_counts)

from imblearn.over_sampling import RandomOverSampler

X = df.drop(columns=['Unnamed: 0', 'sentiment'])  # Features
y = df['sentiment']  # Target variable

# Instantiate RandomOverSampler
oversampler = RandomOverSampler(random_state=42)

# Resample the dataset
X_resampled, y_resampled = oversampler.fit_resample(X, y)

# Create a new DataFrame with the resampled data
resampled_df = pd.DataFrame(X_resampled, columns=X.columns)
resampled_df['sentiment'] = y_resampled

# Check the class distribution after resampling
print(resampled_df['sentiment'].value_counts())

# Function to remove emojis from text
def remove_emojis(text):
    # Define regex pattern to match emojis
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002500-\U00002BEF"  # chinese char
                           u"\U00002702-\U000027B0"
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           u"\U0001f926-\U0001f937"
                           u"\U00010000-\U0010ffff"
                           u"\u2640-\u2642"
                           u"\u2600-\u2B55"
                           u"\u200d"
                           u"\u23cf"
                           u"\u23e9"
                           u"\u231a"
                           u"\ufe0f"  # dingbats
                           u"\u3030"
                           "\U0001F600-\U0001F64F"  # emoticons
                           "\U0001F300-\U0001F5FF"  # symbols & pictographs
                           "\U0001F680-\U0001F6FF"  # transport & map symbols
                           "\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           "\U00002500-\U00002BEF"  # chinese char
                           "\U00002702-\U000027B0"
                           "\U00002702-\U000027B0"
                           "\U000024C2-\U0001F251"
                           "\U0001f926-\U0001f937"
                           "\U00010000-\U0010ffff"
                           "\u2640-\u2642"
                           "\u2600-\u2B55"
                           "\u200d"
                           "\u23cf"
                           "\u23e9"
                           "\u231a"
                           "\ufe0f"  # dingbats
                           "\u3030"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)  # Remove emojis

# Apply the function to the 'text' column
resampled_df['text'] = resampled_df['text'].apply(remove_emojis)

resampled_df

# Define a function to remove special characters and non-alphanumeric characters from Arabic text
def remove_special_characters_arabic(text):
    # Remove special characters and non-alphanumeric characters
    cleaned_text = re.sub('[!@#$:).;,?؟&%]', ' ', text.lower())
    cleaned_text = re.sub('[a-zA-Z]', ' ', cleaned_text)
    cleaned_text = re.sub("['''`÷×؛<>_()*&^%ـ،/:؟.,'{}~¦+|!–''']", ' ', cleaned_text)
    cleaned_text = re.sub('  ', ' ', cleaned_text)
    cleaned_text = re.sub('…', ' ', cleaned_text)
    cleaned_text = re.sub(r'…', ' ', cleaned_text)
    cleaned_text = re.sub(r'_', ' ', cleaned_text)
    cleaned_text = re.sub(r'\\\)', ' ', cleaned_text)
    cleaned_text = re.sub(r'،', ' ', cleaned_text)
    cleaned_text = re.sub(r'"', ' ', cleaned_text)
    cleaned_text = re.sub(r'-', ' ', cleaned_text)
    cleaned_text = re.sub(r'؛', ' ', cleaned_text)
    cleaned_text = re.sub(r'\d+', '', cleaned_text)
    cleaned_text = re.sub(r'\/', ' ', cleaned_text)
    cleaned_text = re.sub(' +', ' ', cleaned_text)
    cleaned_text = re.sub('\n', ' ', cleaned_text)
    cleaned_text = re.sub('أنا', 'انا', cleaned_text)
    cleaned_text = re.sub('☻ ', '', cleaned_text)
    cleaned_text = re.sub('•', '', cleaned_text)
    cleaned_text = re.sub('“', '', cleaned_text)
    cleaned_text = re.sub('”', '', cleaned_text)
    cleaned_text = re.sub(' ﴾', '', cleaned_text)
    cleaned_text = re.sub('٪', '', cleaned_text)
    cleaned_text = re.sub(r'\)', '', cleaned_text)  # Escaping the closing parenthesis
    cleaned_text = re.sub(r'\(', '', cleaned_text)  # Escaping the opening parenthesis
    cleaned_text = re.sub(r'\*', '', cleaned_text)  # Escaping the asterisk
    cleaned_text = re.sub(r'\+', '', cleaned_text)  # Escaping the plus sign
    cleaned_text = re.sub(r'=', '', cleaned_text)  # Escaping the equal sign
    cleaned_text = re.sub(r'\[', '', cleaned_text)  # Escaping the opening square bracket
    cleaned_text = re.sub(r'\]', '', cleaned_text)  # Escaping the closing square bracket
    cleaned_text = re.sub('~', '', cleaned_text)
    cleaned_text = re.sub('》', '', cleaned_text)
    cleaned_text = re.sub('《', '', cleaned_text)
    cleaned_text = re.sub('⚘', '', cleaned_text)
    cleaned_text = re.sub('♡', '', cleaned_text)
    cleaned_text = re.sub('—', '', cleaned_text)
    cleaned_text = re.sub('\u2060', '', cleaned_text)
    cleaned_text = re.sub('', '', cleaned_text)
    cleaned_text = re.sub(' ️️', '', cleaned_text)
    cleaned_text = re.sub('»', '', cleaned_text)
    cleaned_text = re.sub('«', '', cleaned_text)
    cleaned_text = re.sub('\^', '', cleaned_text)  # Escaping the caret
    cleaned_text = re.sub('>>', '', cleaned_text)
    cleaned_text = re.sub('<<', '', cleaned_text)
    cleaned_text = re.sub('}', '', cleaned_text)
    cleaned_text = re.sub('{', '', cleaned_text)
    cleaned_text = re.sub(' ̮', '', cleaned_text)
    cleaned_text = re.sub('>', '', cleaned_text)
    cleaned_text = re.sub('◡̈⃝', '', cleaned_text)
    cleaned_text = re.sub('⚚', '', cleaned_text)
    cleaned_text = re.sub('﴿', '', cleaned_text)
    cleaned_text = re.sub(r'\\', '', cleaned_text)  # Escaping the backslash
    cleaned_text = re.sub('xDxD', '', cleaned_text)

    cleaned_text = re.sub('ّّّّّ', '', cleaned_text)
    cleaned_text = re.sub('⇢ ⇠', '', cleaned_text)
    cleaned_text = re.sub('↷', '', cleaned_text)
    cleaned_text = re.sub('o', '', cleaned_text)
    cleaned_text = re.sub('⇢ ⇠', '', cleaned_text)
    cleaned_text = re.sub('re', '', cleaned_text)
    cleaned_text = re.sub('\r\r \r\r ', '', cleaned_text)
    cleaned_text = re.sub('\r\r', '', cleaned_text)

    return cleaned_text

# Apply the function to remove special characters to the 'text' column
resampled_df['text'] = resampled_df['text'].apply(remove_special_characters_arabic)

resampled_df.head()

"""### Data Tokenization:"""

# Tokenize the text in the 'clean_text' column
resampled_df['tokens'] = resampled_df['text'].apply(word_tokenize)

resampled_df

# Define Arabic stopwords
arb_stopwords = set(nltk.corpus.stopwords.words("arabic"))

# Define a function to remove stopwords from tokenized text
def remove_stopwords(tokens):
    # Remove stopwords from the tokenized text
    tokens_without_stopwords = [word for word in tokens if word not in arb_stopwords]
    return tokens_without_stopwords

# Apply the function to remove stopwords to the 'Tweet_tokenized' column
resampled_df['tweet_nonstop'] = resampled_df['tokens'].apply(remove_stopwords)

# Define additional stopwords
more_stopwords = {'،', 'و', 'على', 'في', 'حتى', 'كل', 'ولا', 'الا', 'ما', 'هذا',
                  'أو', 'ذا', 'حين', 'ليت', 'شي', 'الاتي', 'الله', 'أو', 'عنا',
                  'والله', 'قبل', 'خلال', 'ان', 'اللي', 'او', 'انا', 'بـ', 'شيء',
                  'فقط', 'يوم', 'علينا', 'كان', 'اذا', 'عشان', 'وش', 'وبسبب'}

# Update the Arabic stopwords set with additional stopwords
arb_stopwords = arb_stopwords.union(more_stopwords)

resampled_df

# Initialize Arabic stemmer
stemmer = ISRIStemmer()

# Define a function to perform stemming on Arabic text
def stem_arabic_text(text):
    stemmed_words = [stemmer.stem(word) for word in text.split()]
    return ' '.join(stemmed_words)

# Apply stemming to the 'text' column
resampled_df['stemmed_text'] = resampled_df['text'].apply(stem_arabic_text)

resampled_df

# Function to lemmatize an Arabic token
def lemmatize_arabic_token(token):
    # Define a list of common prefixes and suffixes to remove
    prefixes = ['ال', 'و', 'ف', 'ب', 'ل', 'ي']
    suffixes = ['وا','ه', 'ها', 'ك','م', 'كم', 'هم', 'نا', 'ي', 'هن', 'ات', 'ون', 'ين','ة']

    # Remove prefixes and suffixes
    for prefix in prefixes:
        if token.startswith(prefix):
            token = token[len(prefix):]
            break
    for suffix in suffixes:
        if token.endswith(suffix):
            token = token[:-len(suffix)]
            break

    return token

# Function to lemmatize tokens in a list
def lemmatize_arabic_tokens(tokens):
    lemmatized_tokens = [lemmatize_arabic_token(token) for token in tokens]
    return lemmatized_tokens

# Apply lemmatization to DataFrame
resampled_df['lemmatized_text'] = resampled_df['tokens'].apply(lemmatize_arabic_tokens)

resampled_df

# Join the lemmatized tokens into strings
resampled_df['lemmatized_text_str'] = resampled_df['lemmatized_text'].apply(lambda x: ' '.join(x))
resampled_df

"""### Data Vectorization:


"""

#Step 1: Calculate TF-IDF features for each document
tfidf_vectorizer = TfidfVectorizer()
X_tfidf = tfidf_vectorizer.fit_transform(resampled_df['lemmatized_text_str'])

from gensim.models import Word2Vec

#Training Word2Vec model
word2vec_model = Word2Vec(sentences=resampled_df['lemmatized_text_str'], vector_size=100, window=5, min_count=1, workers=4)
word_vectors = word2vec_model.wv

# Calculate the average Word2Vec representation for each text
def get_average_word2vec(tokens):
    word_vectors = []
    for token in tokens:
        if token in word2vec_model.wv:
            word_vectors.append(word2vec_model.wv[token])
    if word_vectors:
        return np.mean(word_vectors, axis=0)
    else:
        # If none of the tokens are in the vocabulary, return a zero vector
        return np.zeros(word2vec_model.vector_size)

# Apply the function to each tokenized text
resampled_df['avg_word2vec'] = resampled_df['lemmatized_text_str'].apply(get_average_word2vec)

resampled_df

# Convert Word2Vec feature matrix to a dense array
X_word2vec_dense = np.array(resampled_df['avg_word2vec'].to_list())

from scipy.sparse import hstack, csr_matrix

# Convert Word2Vec dense array to a sparse matrix
X_word2vec_sparse = csr_matrix(X_word2vec_dense)

# Concatenate TF-IDF and Word2Vec feature matrices
X_combined = hstack([X_tfidf, X_word2vec_sparse])

resampled_df

X_combined.toarray()

"""### Data Splitting:

"""

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)

"""## Select the NLP methods for machine learning and deep learning-based tasks

**Machine Learning-Based NLP Methods:**
"""

from sklearn.linear_model import LogisticRegression

# Initialize and train the Logistic Regression model
log_reg_model = LogisticRegression(max_iter=1000)
log_reg_model.fit(X_train, y_train)

# Predictions on the testing set
y_pred = log_reg_model.predict(X_test)

"""**Deep Learning-Based NLP Methods:**"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Define the input shape (number of features)
input_shape = X_combined.shape[1]

# Define the model architecture
model = Sequential([
    Dense(128, activation='relu', input_shape=(input_shape,)),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Print the model summary
model.summary()

# Train the model
history = model.fit(X_combined, y, epochs=10, batch_size=32, validation_split=0.2)

"""## Evaluation pipeline set up"""

# Evaluate Logistic Regression model
log_reg_accuracy = accuracy_score(y_test, y_pred)
print("Logistic Regression Accuracy:", log_reg_accuracy)

# Evaluate the DNN model
loss, accuracy = model.evaluate(X_combined, y)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

"""1. **Baseline Algorithm:**
The baseline algorithm combines TF-IDF features with Word2Vec embeddings for each document.
The classifier used for sentiment classification: Logistic Regression model.
1. **Implementation Platform:**
Language: Python
Libraries: NumPy, pandas, scikit-learn
Platform: Kaggle Notebook .
1. **Results:**
Logistic Regression Model:
* Accuracy: 75.97%

Deep Neural Network (DNN) Model:
* Test Loss: 0.22695
* Test Accuracy: 91.04%

**Conclusion:**
* The Deep Neural Network (DNN) model significantly outperforms the Logistic Regression model for sentiment analysis, achieving a higher accuracy of 91.04% compared to 75.97%. This indicates that the DNN model is better at capturing complex patterns in the data and provides more accurate predictions for sentiment classification.

"""